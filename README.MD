# Simple LLM Mechanisms Demo

I've created a simple Jupyter notebook that demonstrates the key mechanisms behind Large Language Models! Here's what this code covers:

## ðŸ”‘ Key Concepts Demonstrated:

1. **Tokenization** - How text gets broken into pieces
2. **Vocabulary** - Converting tokens to numbers
3. **Embeddings** - Turning numbers into meaningful vectors
4. **Attention** - How tokens "pay attention" to each other
5. **Next Token Prediction** - The core of how LLMs work
6. **Text Generation** - Putting it all together

## ðŸŽ¯ What Makes This Simple:

* Uses basic Python operations (no complex libraries)
* Small vocabulary and embedding dimensions
* Clear, commented code with examples
* Visual demonstrations of attention weights
* Step-by-step explanations

## ðŸš€ To Run This:

Just copy the code into a Jupyter notebook and run each cell. You'll see:

* How "the cat sat" becomes numbers and vectors
* How attention weights show which words are important
* A simple text generator in action
* Visualizations of the attention mechanism

The code is intentionally simplified to show the concepts clearly. Real LLMs like GPT use billions of parameters, transformer architectures, and massive datasets, but the core principles are the same as shown in this demo!